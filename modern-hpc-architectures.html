
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Modern HPC architectures</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=84ace793992934648b4de8eed757e5a2" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx_lesson.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx_charts/charts.css" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="_static/overrides.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/minipres.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/sphinx_charts/plotly/plotly-2.8.3.min.js"></script>
    <script src="_static/sphinx_charts/charts.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.9d8b4a8b9bb19db25eeaddc40d639ba2.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://3Dmol.org/build/3Dmol-min.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Performance theory" href="performance-theory.html" />
    <link rel="prev" title="First steps with VeloxChem" href="notebooks/first-steps.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<div class="col-12 col-md-3 bd-sidebar site-navigation " id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/ENCCS-PDC-logos.jpg" class="logo" alt="logo">
      
      
    </a>
</div><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   VeloxChem: quantum chemistry towards pre-exascale and beyond
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="setup.html">
   Setting up your system
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hpc-setup.html">
   Setting up VeloxChem on a HPC cluster
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The lesson
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/first-steps.html">
   First steps with VeloxChem
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Modern HPC architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="performance-theory.html">
   Performance theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="scf-scaling-study.html">
   Scaling study: self-consistent field calculations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="eri-overview.html">
   An overview of electron-repulsion integral evaluation algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linrsp-scaling-study.html">
   Scaling study: excitation energies with linear response
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="x-ray-cpp.html">
   Complex polarization propagator in the X-ray region
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exciton.html">
   Exciton calculation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ntos.html">
   Natural transition orbitals
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="quick-reference.html">
   Quick Reference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="zbibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="guide.html">
   Instructorâ€™s guide
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<!-- This is an invisible pixel that we watch to see if we've scrolled. -->
<div class="sbt-scroll-pixel-helper"></div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            <div class="topbar-left">
                
                <label class="nav-toggle-button" for="__navigation">
                    <div class="visually-hidden">Toggle navigation</div>
                    <i class="fas fa-bars"></i>
                </label>
                
            </div>
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/modern-hpc-architectures.rst.txt"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.rst</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ENCCS/veloxchem-hpc"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/ENCCS/veloxchem-hpc/issues/new?title=Issue%20on%20page%20%2Fmodern-hpc-architectures.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/ENCCS/veloxchem-hpc/edit/main/content/modern-hpc-architectures.rst"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#moore-s-law">
   Mooreâ€™s law
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-memory-hierarchy">
   The memory hierarchy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiprocessor-systems-and-non-uniform-memory-access">
   Multiprocessor systems and non-uniform memory access
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-dardel-system-at-pdc">
   The Dardel system at PDC
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Modern HPC architectures</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#moore-s-law">
   Mooreâ€™s law
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-memory-hierarchy">
   The memory hierarchy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiprocessor-systems-and-non-uniform-memory-access">
   Multiprocessor systems and non-uniform memory access
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-dardel-system-at-pdc">
   The Dardel system at PDC
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="section" id="modern-hpc-architectures">
<span id="id1"></span><h1>Modern HPC architectures<a class="headerlink" href="#modern-hpc-architectures" title="Permalink to this headline">Â¶</a></h1>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Understand how modern HPC hardware is built and why.</p></li>
<li><p>Learn about the memory hierarchy on <a class="reference external" href="https://www.pdc.kth.se/hpc-services/computing-systems/dardel-1.1043529">Dardel</a>
and <a class="reference external" href="https://www.lumi-supercomputer.eu/">LUMI</a>.</p></li>
</ul>
</div>
<p>Quantum chemistry has evolved hand-in-hand with advances in computer
hardware: each new computing architecture that becomes available allows leaps
both in the size of molecular systems that can be studied and the methods that
can be applied.</p>
<p>All hardware available today, from your phone to any large-scale data center, is
engineered to be parallel. Multiple cores, multiple threads, multiple vector
lanes are all, to lesser or greater extent, <strong>built-in</strong> the hardware we have at
our disposal.
The evolution of the hardware has brought a paradigm shift in software
development. It is <em>mandatory</em> for developers to explicitly think about
parallelism and how to harness it to improve performance. Most importantly,
applications need to be <em>scalable</em> in order to exploit all hardware resources
available today, but also in the future, as more parallel resources are packed
into chips.</p>
<div class="section" id="moore-s-law">
<h2>Mooreâ€™s law <a class="footnote-reference brackets" href="#id4" id="id2">*</a><a class="headerlink" href="#moore-s-law" title="Permalink to this headline">Â¶</a></h2>
<p>Back in 1965, Gordon Moore observed that the number of transistors in a dense
integrated circuit doubles about every two years. This observation has been since dubbed <strong>Mooreâ€™s law</strong>.
Being able to pack more transistors means smaller size of a single element, such
that higher clock rates can be achieved.
Higher clock rates mean higher instruction throughput and thus higher
performance.</p>
<p>If we look at the historical trends in the past 50 years of microprocessor
development, we indeed notice the doubling in transistor numbers asserted in
Mooreâ€™s law. It is also quite clear that in the early 2000s the corresponding
increase in clock rates stalled: modern chips have clock rates in the 3 GHz
range.  Code will not be more performant on newer architectures without
<em>significant</em> programmer intervention.</p>
<div class="sphinx-charts docutils container" id="id6">
<div class="sphinx-charts-chart sphinx-charts-chart-id-0 sphinx-charts-chart-uri-_charts/charts/microprocessor-trend-data.json sphinx-charts-chart-dn-chart docutils container" id="sphinx-charts-chart-id-0">
</div>
<p class="sphinx-charts-hidden"></p>
<div class="sphinx-charts-placeholder sphinx-charts-placeholder-0 docutils container" id="id7">
<p class="caption"><span class="caption-text">Loadingâ€¦</span></p>
</div>
<p class="caption"><span class="caption-text">The evolution of microprocessors. The number of transistors per chip
increases every 2 years or so. However, it can no longer be exploited by the
core frequency due to power consumption limits. Before 2000, the increase in
the single core clock frequency was the major source of performance increase.
Mid-2000 mark a transition towards multi-core processors. <a class="footnote-reference brackets" href="#id5" id="id3">â€ </a></span></p>
</div>
<p>What happened? Processor designers hit three walls:</p>
<ul class="simple">
<li><p><strong>Power consumption wall</strong>. This scales as the third power of the clock rate and
the heat generated by a denser transistor packing cannot be dissipated
effectively by air cooling alone. Higher clock rates would result in
power-inefficient computation.</p></li>
<li><p><strong>Memory wall</strong>. While clock rates for computing chips have seen a
steady rise in the past decades, the same has not been the case for memory,
especially that residing <em>off-chip</em>. Read/write operations in memory that is
far away in time and space from the computing chips is expensive, both in
terms of time and required power. Algorithms bound by memory performance will
not gain from faster computation.</p></li>
<li><p><strong>Instruction-level parallelism wall</strong>. Automatic parallelization at the
instruction level, though superscalar instructions, pipelining, prefetching,
and branch prediction, leads to more complicated chip designs and only affords
<em>constant</em>, not <em>scalable</em>, factors of speedup.</p></li>
</ul>
<p>To compensate, chip designs to achieve performance have increased the number of
physical cores on the same die.  More cores, more threads, and wider vector
instruction sets all contribute to expose more <em>hardware parallelism</em> for
applications to take advantage of.</p>
</div>
<div class="section" id="the-memory-hierarchy">
<h2>The memory hierarchy<a class="headerlink" href="#the-memory-hierarchy" title="Permalink to this headline">Â¶</a></h2>
<p>A modern CPU consists of multiple cores on the same chip die. Each core has its
own <strong>arithmetic logic unit</strong> (ALU) and <strong>control unit</strong>.</p>
<div class="figure align-center" id="id8">
<a class="reference internal image-reference" href="_images/cpu.svg"><img alt="_images/cpu.svg" height="400" src="_images/cpu.svg" width="475" /></a>
<p class="caption"><span class="caption-text">A schematic view of a modern multicore CPU. Each purple-shaded box is a
single core, with its own <strong>arithmetic logic unit</strong> (ALU), <strong>control unit</strong>,
and <strong>L1 cache</strong> (yellow-shaded box).  Groups of cores <em>might</em> share the <strong>L2
cache</strong> (blue-shaded box), which is larger and slower than L1. Groups of
cores in the chip share the <strong>L3 cache</strong> (orange-shaded box), in turn larger
and slower than L2. The CPU has access to off-chip <strong>dynamic random access
memory</strong> (DRAM), which is usually of the order of hundreds of gigabytes.
Access to DRAM is much slower than access to caches due to lower memory clock
rates and locality.</span><a class="headerlink" href="#id8" title="Permalink to this image">Â¶</a></p>
</div>
<p>As mentioned, clock rates for memory have not seen the same rise as for
computing chips. This has direct impact on performance: a read/write operation
can require multiple clock cycles, during which the cores might be idle.
This is called <strong>latency</strong> and its impact depends on the <em>locality</em> of the
memory being accessed.
Modern CPUs have a <strong>memory hierarchy</strong>:</p>
<ul class="simple">
<li><p><strong>Registers</strong> are very small and very fast units of memory that store the data
about to be processed by the core.</p></li>
<li><p>multiple caches, with each level the latency typically increases by an order
of magnitude:</p>
<ul>
<li><p><strong>L1 cache</strong> is per-core memory where both instructions and data to be
processed are stored for fast retrieval into the registers. Its size is in the
order of <em>tens of kilobytes</em> per core.</p></li>
<li><p><strong>L2 cache</strong>. Its size is in the order of <em>hundreds of kilobytes</em>
per core and it might be shared by groups of cores.</p></li>
<li><p><strong>L3 cache</strong>. It is shared among cores, either subgroups or all, and its size
is in the order of <em>tens of megabytes</em> per <em>group</em> of cores.</p></li>
</ul>
</li>
<li><p><strong>DRAM</strong> this is the main off-chip memory. Nowadays, HPC clusters have <em>hundreds
of gigabytes</em> of RAM per node. Its latency is usually two order of magnitude
larger than that of L3 cache.</p></li>
</ul>
<p>We can see that the closest the memory is to the core, the faster it can be
accessed. Unfortunately, on-chip memory is rather small.</p>
<p>As an example, the <a class="reference external" href="https://en.wikichip.org/wiki/amd/epyc/7742">AMD EPYC 7742</a> CPUs on Dardel have 64 cores and
cache hierarchy:</p>
<ul class="simple">
<li><p>L1 instruction cache of 32 KiB per core, for a total of 2 MiB.</p></li>
<li><p>L1 data cache of 32 KiB per core, for a total of 2 MiB.</p></li>
<li><p>L2 cache of 512 KiB per core, for a total of 32 MiB.</p></li>
<li><p>L3 cache of 16 MiB shared among 16 cores, for a total of 256 MiB.</p></li>
</ul>
</div>
<div class="section" id="multiprocessor-systems-and-non-uniform-memory-access">
<h2>Multiprocessor systems and non-uniform memory access<a class="headerlink" href="#multiprocessor-systems-and-non-uniform-memory-access" title="Permalink to this headline">Â¶</a></h2>
<p>Multiple multicore CPUs can be packaged together in a <strong>socket</strong>. The CPUs
communicate through fast point-to-point channels. In this architecture, each
CPU in the socket is attached to its own off-chip memory.  As a result, access
to the memory is not equal across CPUs in the socket.</p>
<p>Off-chip memory accesses become <strong>non-uniform</strong>: the CPU on socket 0 (socket 1)
experiences higher latency and, possibly, reduced bandwidth accessing DRAM
attached to the CPU on socket 1 (socket 0).
To further complicate matters, <em>cores</em> on each socket might also be arranged in
<strong>non-uniform memory access</strong> (NUMA) domains. Cores within each socket might
experience different latency and bandwidth when accessing memory.</p>
<div class="figure align-center" id="id9">
<span id="numa"></span><a class="reference internal image-reference" href="_images/numa.svg"><img alt="_images/numa.svg" height="496" src="_images/numa.svg" width="768" /></a>
<p class="caption"><span class="caption-text">Schematic view of a typical dual-socket node on a modern cluster.  Each
socket houses two CPUs, each with 64 cores. The cores are arranged in a
configuration with 4 NUMA domains per socket (NPS4).  Each NUMA domain has 16
cores.</span><a class="headerlink" href="#id9" title="Permalink to this image">Â¶</a></p>
</div>
</div>
<div class="section" id="the-dardel-system-at-pdc">
<h2>The Dardel system at PDC<a class="headerlink" href="#the-dardel-system-at-pdc" title="Permalink to this headline">Â¶</a></h2>
<p>Dardel is the new high-performance cluster at PDC: it has a CPU <em>partition</em> and
a GPU <em>partition</em> is planned.</p>
<img alt="https://www.pdc.kth.se/polopoly_fs/1.1053343.1614296818!/image/3D%20marketing%201%20row%20cropped%201000pW%20300ppi.jpg" class="align-center" src="https://www.pdc.kth.se/polopoly_fs/1.1053343.1614296818!/image/3D%20marketing%201%20row%20cropped%201000pW%20300ppi.jpg" />
<p>Anatomy of supercomputer:</p>
<ul class="simple">
<li><p>Dardel consists of several <em>cabinets</em> (also known as racks)</p></li>
<li><p>Each cabinet is filled with many <em>blades</em></p></li>
<li><p>A single blade hosts two <em>nodes</em></p></li>
<li><p>A node has two AMD EPYC 7742 CPUs, each with 64 cores clocking at 2.25GHz</p></li>
</ul>
<p>Different types of compute nodes in the CPU partition:</p>
<ul class="simple">
<li><p>488 x 256 GB (SNIC thin nodes)</p></li>
<li><p>20 x 512 GB (SNIC large nodes)</p></li>
<li><p>8 x 1024 GB (SNIC huge nodes)</p></li>
<li><p>2 x 2048 GB (SNIC giant nodes)</p></li>
<li><p>36 x 256 GB (KTH industry/business research nodes)</p></li>
</ul>
<p>The performance of the CPU partition is 2.279 petaFLOPS according to the Top500
list (Nov 2021).</p>
<div class="admonition-exploring-the-memory-hierarchy-on-dardel typealong toggle-shown dropdown admonition" id="typealong-0">
<p class="admonition-title">Exploring the memory hierarchy on Dardel</p>
<p>Each of Dardelâ€™s node is dual-socket: the memory latency and bandwidth will
differ based on which CPU/core accesses the off-chip memory.
We will use the <a class="reference external" href="https://linux.die.net/man/8/numactl">numactl</a> command-line
tool to get a description of the NUMA domains on the Dardel login and compute
nodes.</p>
<p>To do so:</p>
<ul>
<li><p>Log in to Dardel:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ssh &lt;your-username&gt;@dardel.pdc.kth.se
</pre></div>
</div>
</li>
<li><p>Run the command on the login node:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>numactl --hardware
</pre></div>
</div>
</li>
<li><p>Request a short interactive allocation and run the command on a compute
node:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>salloc -N <span class="m">1</span> -t <span class="m">00</span>:05:00 -p main -A edu22.veloxchem --reservation velox-lab1
srun -n <span class="m">1</span> numactl --hardware
<span class="nb">exit</span>
</pre></div>
</div>
</li>
</ul>
<p>Note that the login node and the compute node give very different output.</p>
<p>On the log in node, the output looks like the following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
node 0 size: 257342 MB
node 0 free: 70756 MB
node 1 cpus: 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255
node 1 size: 258019 MB
node 1 free: 49565 MB
node distances:
node   0   1
  0:  10  32
  1:  32  10
</pre></div>
</div>
<p>While on the compute node, the output looks like the following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>available: 8 nodes (0-7)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 0 size: 31620 MB
node 0 free: 30673 MB
node 1 cpus: 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159
node 1 size: 32249 MB
node 1 free: 31150 MB
node 2 cpus: 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175
node 2 size: 32249 MB
node 2 free: 30757 MB
node 3 cpus: 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
node 3 size: 32237 MB
node 3 free: 31752 MB
node 4 cpus: 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207
node 4 size: 32249 MB
node 4 free: 31783 MB
node 5 cpus: 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223
node 5 size: 32249 MB
node 5 free: 31813 MB
node 6 cpus: 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239
node 6 size: 32249 MB
node 6 free: 31198 MB
node 7 cpus: 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255
node 7 size: 32246 MB
node 7 free: 31375 MB
node distances:
node   0   1   2   3   4   5   6   7
  0:  10  12  12  12  32  32  32  32
  1:  12  10  12  12  32  32  32  32
  2:  12  12  10  12  32  32  32  32
  3:  12  12  12  10  32  32  32  32
  4:  32  32  32  32  10  12  12  12
  5:  32  32  32  32  12  10  12  12
  6:  32  32  32  32  12  12  10  12
  7:  32  32  32  32  12  12  12  10
</pre></div>
</div>
<p>Since the actual calculations will be run on the compute nodes,
we need to take a close look at the <code class="docutils literal notranslate"><span class="pre">numactl</span></code> output.</p>
<ol class="arabic">
<li><p>There are 8 NUMA domains: <code class="docutils literal notranslate"><span class="pre">available:</span> <span class="pre">8</span> <span class="pre">nodes</span> <span class="pre">(0-7)</span></code></p></li>
<li><p>The index for the threads in the domain 0, together with the total and
free amounts of memory.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 0 size: 31620 MB
node 0 free: 30673 MB
</pre></div>
</div>
</li>
<li><p>The index for the threads in the domain 1, together with the total and
free amounts of memory.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>node 1 cpus: 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159
node 1 size: 32249 MB
node 1 free: 31150 MB
</pre></div>
</div>
</li>
<li><p>The distances between nodes. These numbers give a measure of the latency
incurred accessing memory on one NUMA domain from the other.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>node distances:
node   0   1   2   3   4   5   6   7
  0:  10  12  12  12  32  32  32  32
  1:  12  10  12  12  32  32  32  32
  2:  12  12  10  12  32  32  32  32
  3:  12  12  12  10  32  32  32  32
  4:  32  32  32  32  10  12  12  12
  5:  32  32  32  32  12  10  12  12
  6:  32  32  32  32  12  12  10  12
  7:  32  32  32  32  12  12  12  10
</pre></div>
</div>
</li>
</ol>
</div>
<div class="admonition-efficient-utilization-of-resources-hardware-on-dardel typealong toggle-shown dropdown admonition" id="typealong-1">
<p class="admonition-title">Efficient utilization of resources/hardware on Dardel</p>
<p>Nowadays more and more scientific software are parallelized over
both MPI and OpenMP.
When running hybrid MPI/OpenMP software on multi-core compute nodes,
there are many choices in the combination of MPI processes and OpenMP
threads.</p>
<p>On Dardel this can be controlled by the SLURM job script, where user
can specify the number of MPI tasks per node. The OpenMP threads can
be controlled by relevant environment variables such as
<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> and <code class="docutils literal notranslate"><span class="pre">OMP_PLACES</span></code>.</p>
<p>Here are some examples of requesting different combinations of MPI
tasks and OpenMP threads on Dardel. Please note that <code class="docutils literal notranslate"><span class="pre">--cpus-per-task</span></code>
should be set to 2x <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> because simultaneous multithreading
(SMT) is turned on.</p>
<ul>
<li><p>128 MPI x 1 OMP</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=128</span>

<span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span> <span class="nv">OMP_PLACES</span><span class="o">=</span>cores
</pre></div>
</div>
</li>
<li><p>64 MPI x 2 OMP</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=64</span>
<span class="c1">#SBATCH --cpus-per-task=4  # 2x2 because of SMT</span>

<span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">2</span>
<span class="nb">export</span> <span class="nv">OMP_PLACES</span><span class="o">=</span>cores
</pre></div>
</div>
</li>
<li><p>8 MPI x 16 OMP</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=8</span>

<span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">16</span>
<span class="nb">export</span> <span class="nv">OMP_PLACES</span><span class="o">=</span>cores
</pre></div>
</div>
</li>
<li><p>2 MPI x 64 OMP</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=2</span>
<span class="c1">#SBATCH --cpus-per-task=128  # 64x2 because of SMT</span>

<span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">64</span>
<span class="nb">export</span> <span class="nv">OMP_PLACES</span><span class="o">=</span>cores
</pre></div>
</div>
</li>
<li><p>1 MPI x 128 OMP</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>

<span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">128</span>
<span class="nb">export</span> <span class="nv">OMP_PLACES</span><span class="o">=</span>cores
</pre></div>
</div>
</li>
</ul>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>It is not possible to achieve higher clock rates: more performing hardware
packs multiple computational cores on the same die.</p></li>
<li><p>Multicore machines give us access to more parallelism, but this needs to be
harnessed with careful software design.</p></li>
<li><p>Understanding the existing memory hierarchy is essential for efficient use
of the hardware.</p></li>
</ul>
</div>
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id2">*</a></span></dt>
<dd><p>This section is adapted, with permission, from the training material for
the <a class="reference external" href="https://enccs.github.io/CUDA/1.01_GPUIntroduction/#exposing-parallelism">ENCCS CUDA workshop</a>.</p>
</dd>
<dt class="label" id="id5"><span class="brackets"><a class="fn-backref" href="#id3">â€ </a></span></dt>
<dd><p>The data in this plot is collected by Karl Rupp and made available <a class="reference external" href="https://github.com/karlrupp/microprocessor-trend-data">on GitHub</a>.</p>
</dd>
</dl>
</div>
</div>


              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="notebooks/first-steps.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">First steps with VeloxChem</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="performance-theory.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Performance theory</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The contributors<br/>
    
        &copy; Copyright 2022, The contributors.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>